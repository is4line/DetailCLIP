{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd16974",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'timm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransforms\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_dataset\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleTokenizer\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AverageMeter, ProgressMeter, accuracy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/DetailCLIP/models.py:8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtimm\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'timm'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "# import subprocess\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    wandb = None\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from datasets import get_dataset\n",
    "import models\n",
    "from tokenizer import SimpleTokenizer\n",
    "from utils import AverageMeter, ProgressMeter, accuracy\n",
    "import utils\n",
    "from torchvision.datasets import ImageFolder\n",
    "from utils import GaussianBlur, Solarize\n",
    "from losses import DetailCLIPLoss, get_metric_names\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780979ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f2bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_zeroshot(val_loader, model, tokenizer, ema=False):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    print('=> encoding captions')\n",
    "    cwd = os.path.dirname(os.path.realpath('test_ckp.ipynb'))\n",
    "    with open(os.path.join(cwd, 'templates.json')) as f:\n",
    "        templates = json.load(f)['fer2013']\n",
    "\n",
    "    with open(os.path.join(cwd, 'labels.json')) as f:\n",
    "        labels = json.load(f)['fer2013']\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = []\n",
    "        for l in labels:\n",
    "            texts = [t.format(l) for t in templates]\n",
    "            texts = tokenizer(texts).cuda(device, non_blocking=True)\n",
    "            class_embeddings = utils.get_model(model).encode_text(texts, ema=ema)\n",
    "            class_embeddings = class_embeddings / class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embeddings = class_embeddings.mean(dim=0)\n",
    "            class_embeddings = class_embeddings / class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            text_features.append(class_embeddings)\n",
    "        text_features = torch.stack(text_features, dim=0)\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.cuda(device, non_blocking=True)\n",
    "            target = target.cuda(device, non_blocking=True)\n",
    "\n",
    "            # encode images\n",
    "            image_features = utils.get_model(model).encode_image(images, ema=ema)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # cosine similarity as logits\n",
    "            logits_per_image = image_features @ text_features.t()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(logits_per_image, target, topk=(1, 5))\n",
    "            acc1, acc5 = utils.scaled_all_reduce([acc1, acc5])\n",
    "            top1.update(acc1.item(), images.size(0))\n",
    "            top5.update(acc5.item(), images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # if i % args.print_freq == 0:\n",
    "            #     progress.display(i)\n",
    "\n",
    "    progress.synchronize()\n",
    "    print('0-shot * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    return {'acc1': top1.avg, 'acc5': top5.avg}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe83155",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "                                     \n",
    "val_transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "cwd = os.path.dirname(os.path.realpath('test_ckp.ipynb'))\n",
    "with open(os.path.join(cwd, 'dataset_catalog.json')) as f:\n",
    "        root = json.load(f)['fer2013']['path']\n",
    "\n",
    "#add val folder for imagenet 1k\n",
    "val_dataset = ImageFolder(os.path.join(root, 'test'), val_transform)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=5, shuffle=None,\n",
    "        num_workers=1, pin_memory=True, sampler=None, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed48708",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d67557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreating MAE projection head\n",
      "\tMAE projection head created\n",
      "\tCreating IBOT projection head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tkeys have been loaded for ibot head with status: <All keys matched successfully>\n",
      "\tIBOT projection head created\n",
      "\tDetailCLIP model created\n",
      "=> creating model: ICLIP_VITB16\n",
      "=> loaded resume checkpoint 'DETAILCLIP_VITB16' (epoch 48)\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = 'checkpoint_best.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "model_name='DETAILCLIP_VITB16'\n",
    "model = getattr(models, model_name)()\n",
    "model.cuda()\n",
    "\n",
    "# create model\n",
    "old_args = ckpt['args']\n",
    "print(\"=> creating model: {}\".format(old_args.model))\n",
    "\n",
    "state_dict = OrderedDict()\n",
    "for k, v in ckpt['state_dict'].items():\n",
    "    state_dict[k.replace('module.', '')] = v\n",
    "\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "print(\"=> loaded resume checkpoint '{}' (epoch {})\".format(model_name, ckpt['epoch']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a9102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> encoding captions\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "VisionTransformer.forward() got an unexpected keyword argument 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvalidate_zeroshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mvalidate_zeroshot\u001b[39m\u001b[34m(val_loader, model, tokenizer, ema)\u001b[39m\n\u001b[32m     37\u001b[39m target = target.cuda(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# encode images\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m image_features = \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m image_features = image_features / image_features.norm(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# cosine similarity as logits\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/DetailCLIP/models.py:652\u001b[39m, in \u001b[36mDetailCLIP.encode_image\u001b[39m\u001b[34m(self, image, mask, ret, ema)\u001b[39m\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, mask=\u001b[38;5;28;01mNone\u001b[39;00m, ret=\u001b[38;5;28;01mFalse\u001b[39;00m, ema=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    651\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ema == \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m         x, attn, ids_restore, mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m         tokens = x\n\u001b[32m    654\u001b[39m         x = x[:, \u001b[32m0\u001b[39m] @ \u001b[38;5;28mself\u001b[39m.image_projection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: VisionTransformer.forward() got an unexpected keyword argument 'mask'"
     ]
    }
   ],
   "source": [
    "print(validate_zeroshot(val_loader, model, tokenizer, ema=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dbd93",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DefaultCfg' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m ckpt = torch.load(ckpt_path, map_location=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     10\u001b[39m old_args = ckpt[\u001b[33m'\u001b[39m\u001b[33margs\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m model.cuda()\n\u001b[32m     14\u001b[39m model.load_state_dict(state_dict, strict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/DetailCLIP/models.py:1040\u001b[39m, in \u001b[36mACLIP_VITB16\u001b[39m\u001b[34m(mask_ratio, **kwargs)\u001b[39m\n\u001b[32m   1039\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mACLIP_VITB16\u001b[39m(mask_ratio=\u001b[32m0\u001b[39m, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m     vision_model = \u001b[43mtimm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmask_vit_base_patch16_224\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_ratio\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1043\u001b[39m     vision_model_ema = timm.create_model(\n\u001b[32m   1044\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmask_vit_base_patch16_224\u001b[39m\u001b[33m'\u001b[39m, num_classes=\u001b[32m0\u001b[39m, mask_ratio=\u001b[32m0\u001b[39m\n\u001b[32m   1045\u001b[39m     )\n\u001b[32m   1046\u001b[39m     model = DetailCLIP(\n\u001b[32m   1047\u001b[39m         embed_dim=\u001b[32m512\u001b[39m,\n\u001b[32m   1048\u001b[39m         vision_width=\u001b[32m768\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1070\u001b[39m         shared_head_teacher = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1071\u001b[39m         **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/timm/models/_factory.py:126\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, cache_dir, scriptable, exportable, no_jit, **kwargs)\u001b[39m\n\u001b[32m    124\u001b[39m create_fn = model_entrypoint(model_name)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable=scriptable, exportable=exportable, no_jit=no_jit):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     model = \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[32m    135\u001b[39m     load_checkpoint(model, checkpoint_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/DetailCLIP/models.py:896\u001b[39m, in \u001b[36mmask_vit_base_patch16_224\u001b[39m\u001b[34m(pretrained, **kwargs)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[33;03mImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\u001b[39;00m\n\u001b[32m    894\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    895\u001b[39m model_kwargs = \u001b[38;5;28mdict\u001b[39m(patch_size=\u001b[32m16\u001b[39m, embed_dim=\u001b[32m768\u001b[39m, depth=\u001b[32m12\u001b[39m, num_heads=\u001b[32m12\u001b[39m, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m model = \u001b[43m_create_vision_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvit_base_patch16_224\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMaskVisionTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/DetailCLIP/models.py:947\u001b[39m, in \u001b[36m_create_vision_transformer\u001b[39m\u001b[34m(variant, transformer, pretrained, default_cfg, **kwargs)\u001b[39m\n\u001b[32m    942\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    943\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfeatures_only not implemented for Vision Transformer models.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    944\u001b[39m     )\n\u001b[32m    946\u001b[39m \u001b[38;5;66;03m# NOTE this extra code to support handling of repr size for in21k pretrained models\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m default_num_classes = \u001b[43mdefault_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_classes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    948\u001b[39m num_classes = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mnum_classes\u001b[39m\u001b[33m\"\u001b[39m, default_num_classes)\n\u001b[32m    949\u001b[39m repr_size = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mrepresentation_size\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: 'DefaultCfg' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "old_args = ckpt['args']\n",
    "\n",
    "model = getattr(models, model_name)()\n",
    "model.cuda()\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "print(\"=> loaded resume checkpoint '{}' (epoch {})\".format(args.resume, ckpt['epoch']))\n",
    "\n",
    "\n",
    "data_path = '/home/onyxia/work/datasets/stanford_dogs'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally resume from a checkpoint (takes precedence over autoresume)\n",
    "\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "state_dict = OrderedDict()\n",
    "for k, v in ckpt['state_dict'].items():\n",
    "    state_dict[k.replace('module.', '')] = v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a6133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c45de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
