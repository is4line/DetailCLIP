{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4081bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from tokenizer import SimpleTokenizer\n",
    "import datasets\n",
    "import utils\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import kaleido\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Subset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "624663ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreating MAE projection head\n",
      "\tMAE projection head created\n",
      "\tCreating IBOT projection head\n",
      "\tkeys have been loaded for ibot head with status: <All keys matched successfully>\n",
      "\tIBOT projection head created\n",
      "\tDetailCLIP model created\n",
      "=> creating model: ICLIP_VITB16\n",
      "\tCreating MAE projection head\n",
      "\tMAE projection head created\n",
      "\tCreating IBOT projection head\n",
      "\tkeys have been loaded for ibot head with status: <All keys matched successfully>\n",
      "\tIBOT projection head created\n",
      "\tDetailCLIP model created\n",
      "=> loaded resume checkpoint 'checkpoint_best.pt' (epoch 48)\n",
      "=> creating dataset\n"
     ]
    }
   ],
   "source": [
    "model = getattr(models, 'ICLIP_VITB16')()\n",
    "model.cuda()\n",
    "\n",
    "# Creating model\n",
    "ckpt_path = 'checkpoint_best.pt'\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "state_dict = OrderedDict()\n",
    "for k, v in ckpt['state_dict'].items():\n",
    "    state_dict[k.replace('module.', '')] = v\n",
    "\n",
    "old_args = ckpt['args']\n",
    "print(\"=> creating model: {}\".format(old_args.model))\n",
    "model = getattr(models, old_args.model)()\n",
    "model.cuda()\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "print(\"=> loaded resume checkpoint '{}' (epoch {})\".format(ckpt_path, ckpt['epoch']))\n",
    "\n",
    "cwd = '/home/onyxia/work/DetailCLIP'\n",
    "with open(os.path.join(cwd, 'dataset_catalog.json')) as f:\n",
    "    catalog = json.load(f)\n",
    "\n",
    "with open(os.path.join(cwd, 'templates.json')) as f:\n",
    "    all_templates = json.load(f)\n",
    "\n",
    "with open(os.path.join(cwd, 'labels.json')) as f:\n",
    "    all_labels = json.load(f)\n",
    "\n",
    "print(\"=> creating dataset\")\n",
    "tokenizer = SimpleTokenizer()\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        lambda x: x.convert('RGB'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0511c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_keep = ['001.Black_footed_Albatross',\n",
    "'002.Laysan_Albatross',\n",
    "'005.Crested_Auklet',\n",
    "'012.Yellow_headed_Blackbird',\n",
    "'016.Painted_Bunting',\n",
    "'017.Cardinal',\n",
    "'019.Gray_Catbird',\n",
    "'036.Northern_Flicker',\n",
    "'044.Frigatebird',\n",
    "'045.Northern_Fulmar',\n",
    "'056.Pine_Grosbeak',\n",
    "'069.Rufous_Hummingbird']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "999e785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "original_classes = [0, 1, 4, 11, 15, 16, 18, 35, 43, 44, 55, 68]\n",
    "class_to_idx = {orig_cls: i for i, orig_cls in enumerate(original_classes)}\n",
    "print(class_to_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88efff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cub200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d='cub200'\n",
    "print('Evaluating {}'.format(d))\n",
    "val_dataset = datasets.get_downstream_dataset(catalog, name=d, is_train=False, transform=val_transform)\n",
    "\n",
    "class_to_idx = {cls: i for i, cls in enumerate(val_dataset.classes)}\n",
    "indices = [i for i, (_, label) in enumerate(val_dataset.samples) \n",
    "           if val_dataset.classes[label] in classes_to_keep]\n",
    "\n",
    "subset = Subset(val_dataset, indices) # dataset with selected classes\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    subset, batch_size=58, shuffle=False,\n",
    "    num_workers=10, pin_memory=True, drop_last=False)\n",
    "\n",
    "labels = [all_labels[d][i] for i in original_classes]\n",
    "templates = all_templates[d]\n",
    "\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fdd3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(val_dataset.root)\n",
    "counts = {cls: len(list((root / cls).glob('*.jpg'))) for cls in classes_to_keep}\n",
    "img_counts = list(counts.values()) # number of images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c27a24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a871093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> encoding captions\n",
      "top1 : tensor(91.3793)\n",
      "top3 : tensor(99.7126)\n",
      "top5 : tensor(100.)\n"
     ]
    }
   ],
   "source": [
    "all_outputs = []\n",
    "all_targets = []\n",
    "all_acc=[]\n",
    "\n",
    "print('=> encoding captions')\n",
    "with torch.no_grad():\n",
    "    text_features = []\n",
    "    for label in labels:\n",
    "        if isinstance(label, list):\n",
    "            texts = [t.format(l) for t in templates for l in label]\n",
    "        else:\n",
    "            texts = [t.format(label) for t in templates]\n",
    "        texts = tokenizer(texts).cuda(non_blocking=True)\n",
    "        texts = texts.view(-1, 77).contiguous()\n",
    "        class_embeddings = utils.get_model(model).encode_text(texts, ema=True)\n",
    "        class_embeddings = class_embeddings / class_embeddings.norm(dim=-1, keepdim=True)\n",
    "        class_embeddings = class_embeddings.mean(dim=0)\n",
    "        class_embeddings = class_embeddings / class_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_features.append(class_embeddings)\n",
    "    text_features = torch.stack(text_features, dim=0)\n",
    "\n",
    "    for images, target in val_loader:\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True)\n",
    "\n",
    "        modified_target = []\n",
    "        for t in target:\n",
    "            modified_target.append(class_to_idx[t.item()])\n",
    "\n",
    "        target = torch.tensor(modified_target, device='cuda')\n",
    "\n",
    "        # encode images\n",
    "        image_features = utils.get_model(model).encode_image(images, ema=True)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logits_per_image = image_features @ text_features.t()\n",
    "\n",
    "        all_outputs.append(logits_per_image)\n",
    "        all_targets.append(target)\n",
    "\n",
    "        #print(logits_per_image.size(), target.size()) # [58,12]  [58]\n",
    "        acc = accuracy(logits_per_image, target, topk=(1,3,5))\n",
    "        all_acc.append(acc)\n",
    "\n",
    "top1 = torch.tensor([a[0] for a in all_acc]).mean()\n",
    "top3 = torch.tensor([a[1] for a in all_acc]).mean()\n",
    "top5 = torch.tensor([a[2] for a in all_acc]).mean()\n",
    "\n",
    "print('top1 :', top1)\n",
    "print('top3 :', top3)\n",
    "print('top5 :', top5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
