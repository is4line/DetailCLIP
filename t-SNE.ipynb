{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb382a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import models\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from tokenizer import SimpleTokenizer\n",
    "import datasets\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0aaee4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreating MAE projection head\n",
      "\tMAE projection head created\n",
      "\tCreating IBOT projection head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tkeys have been loaded for ibot head with status: <All keys matched successfully>\n",
      "\tIBOT projection head created\n",
      "\tDetailCLIP model created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ICLIP(\n",
       "  (visual): MaskVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (pre_logits): Identity()\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (visual_ema): MaskVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (pre_logits): Identity()\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (transformer_e): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_embed): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (reconstruction_pred): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (ibot_head): iBOTHead(\n",
       "    (mlp): CustomSequential(\n",
       "      (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    )\n",
       "    (last_layer): Linear(in_features=256, out_features=8192, bias=False)\n",
       "    (last_layer2): Linear(in_features=256, out_features=8192, bias=False)\n",
       "  )\n",
       "  (ibot_head_e): iBOTHead(\n",
       "    (mlp): CustomSequential(\n",
       "      (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    )\n",
       "    (last_layer): Linear(in_features=256, out_features=8192, bias=False)\n",
       "    (last_layer2): Linear(in_features=256, out_features=8192, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = getattr(models, 'ICLIP_VITB16')()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c24baaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model: ICLIP_VITB16\n",
      "\tCreating MAE projection head\n",
      "\tMAE projection head created\n",
      "\tCreating IBOT projection head\n",
      "\tkeys have been loaded for ibot head with status: <All keys matched successfully>\n",
      "\tIBOT projection head created\n",
      "\tDetailCLIP model created\n",
      "=> loaded resume checkpoint 'checkpoint_best.pt' (epoch 48)\n"
     ]
    }
   ],
   "source": [
    "# Creating model\n",
    "ckpt_path = 'checkpoint_best.pt'\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "state_dict = OrderedDict()\n",
    "for k, v in ckpt['state_dict'].items():\n",
    "    state_dict[k.replace('module.', '')] = v\n",
    "\n",
    "old_args = ckpt['args']\n",
    "print(\"=> creating model: {}\".format(old_args.model))\n",
    "model = getattr(models, old_args.model)()\n",
    "model.cuda()\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "print(\"=> loaded resume checkpoint '{}' (epoch {})\".format(ckpt_path, ckpt['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e1c7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = '/home/onyxia/work/DetailCLIP'\n",
    "with open(os.path.join(cwd, 'dataset_catalog.json')) as f:\n",
    "    catalog = json.load(f)\n",
    "\n",
    "with open(os.path.join(cwd, 'templates.json')) as f:\n",
    "    all_templates = json.load(f)\n",
    "\n",
    "with open(os.path.join(cwd, 'labels.json')) as f:\n",
    "    all_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0f62618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating dataset\n"
     ]
    }
   ],
   "source": [
    "# Data loading code\n",
    "print(\"=> creating dataset\")\n",
    "tokenizer = SimpleTokenizer()\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        lambda x: x.convert('RGB'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80b41460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cub200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for d in catalog:\n",
    "    print('Evaluating {}'.format(d))\n",
    "    val_dataset = datasets.get_downstream_dataset(catalog, name=d, is_train=False, transform=val_transform)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=256, shuffle=False,\n",
    "        num_workers=10, pin_memory=True, drop_last=False)\n",
    "\n",
    "    templates = all_templates[d]\n",
    "    labels = all_labels[d]\n",
    "\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef067a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cub200\n",
      "=> encoding captions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "for d in catalog:\n",
    "    print('Evaluating {}'.format(d))\n",
    "    val_dataset = datasets.get_downstream_dataset(catalog, name=d, is_train=False, transform=val_transform)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=256, shuffle=False,\n",
    "        num_workers=10, pin_memory=True, drop_last=False)\n",
    "\n",
    "    templates = all_templates[d]\n",
    "    labels = all_labels[d]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    print('=> encoding captions')\n",
    "    with torch.no_grad():\n",
    "        text_features = []\n",
    "        for label in labels:\n",
    "            if isinstance(label, list):\n",
    "                texts = [t.format(l) for t in templates for l in label]\n",
    "            else:\n",
    "                texts = [t.format(label) for t in templates]\n",
    "            texts = tokenizer(texts).cuda(non_blocking=True)\n",
    "            texts = texts.view(-1, 77).contiguous()\n",
    "            class_embeddings = utils.get_model(model).encode_text(texts, ema=True)\n",
    "            class_embeddings = class_embeddings / class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embeddings = class_embeddings.mean(dim=0)\n",
    "            class_embeddings = class_embeddings / class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            text_features.append(class_embeddings)\n",
    "\n",
    "        text_features = torch.stack(text_features, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcc1d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_features_tot = []\n",
    "    for images, target in val_loader:\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True)\n",
    "\n",
    "        # encode images\n",
    "        image_features = utils.get_model(model).encode_image(images, ema=True)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        image_features_tot.append(image_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "642f2aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.0066, -0.0634,  0.0728,  ...,  0.0365, -0.0034, -0.0179],\n",
      "        [ 0.0038, -0.0198, -0.0476,  ...,  0.0205,  0.0356,  0.0236],\n",
      "        [ 0.0129, -0.0154,  0.0238,  ...,  0.0551, -0.0033, -0.0015],\n",
      "        ...,\n",
      "        [ 0.0082, -0.0489,  0.0071,  ..., -0.0102, -0.0467, -0.0173],\n",
      "        [-0.0299, -0.0024,  0.0376,  ..., -0.0193, -0.0307,  0.0027],\n",
      "        [-0.0175, -0.0791,  0.0748,  ..., -0.0237, -0.0170,  0.0056]],\n",
      "       device='cuda:0'), tensor([[-0.0261, -0.0602,  0.0404,  ..., -0.0162, -0.0122, -0.0057],\n",
      "        [-0.0436, -0.0608,  0.0129,  ..., -0.0048, -0.0182,  0.0077],\n",
      "        [ 0.0091,  0.0097,  0.0220,  ..., -0.0166, -0.0391,  0.0316],\n",
      "        ...,\n",
      "        [ 0.0579,  0.0024,  0.0385,  ..., -0.0365, -0.0212,  0.0228],\n",
      "        [-0.0039, -0.0065,  0.0295,  ..., -0.0241, -0.0319,  0.0187],\n",
      "        [-0.0215,  0.0091,  0.0095,  ..., -0.0375, -0.0553, -0.0103]],\n",
      "       device='cuda:0'), tensor([[-0.0540, -0.0167,  0.0168,  ..., -0.0026, -0.0569,  0.0190],\n",
      "        [ 0.0058,  0.0106,  0.0182,  ...,  0.0097, -0.0433,  0.0010],\n",
      "        [ 0.0371, -0.0195,  0.0570,  ...,  0.0063, -0.0394, -0.0145],\n",
      "        ...,\n",
      "        [ 0.0324, -0.0159,  0.0744,  ...,  0.0284,  0.0164,  0.0110],\n",
      "        [ 0.0414,  0.0065, -0.0211,  ..., -0.0185,  0.0129, -0.0323],\n",
      "        [-0.0232, -0.0316,  0.0586,  ..., -0.0221,  0.0471,  0.0169]],\n",
      "       device='cuda:0'), tensor([[ 0.0047, -0.0037,  0.0287,  ...,  0.0170,  0.0217,  0.0163],\n",
      "        [ 0.0698, -0.0148, -0.0093,  ..., -0.0103,  0.0415, -0.0145],\n",
      "        [ 0.0519,  0.0352,  0.0079,  ..., -0.0135,  0.0281, -0.0255],\n",
      "        ...,\n",
      "        [-0.0159, -0.0138,  0.0439,  ...,  0.0316, -0.0251, -0.0398],\n",
      "        [-0.0098,  0.0021,  0.0829,  ..., -0.0395, -0.0134, -0.0259],\n",
      "        [-0.0248, -0.0201, -0.0164,  ...,  0.0157, -0.0365, -0.0116]],\n",
      "       device='cuda:0'), tensor([[-0.0517, -0.0154,  0.0108,  ...,  0.0108,  0.0624,  0.0136],\n",
      "        [-0.0056,  0.0084, -0.0113,  ...,  0.0028, -0.0390,  0.0476],\n",
      "        [-0.0051, -0.0582,  0.0375,  ..., -0.0455,  0.0128,  0.0258],\n",
      "        ...,\n",
      "        [ 0.0008, -0.0403,  0.0411,  ...,  0.0426,  0.0018, -0.0123],\n",
      "        [ 0.0003, -0.0287,  0.0223,  ...,  0.0191,  0.0478, -0.0225],\n",
      "        [-0.0841, -0.0254,  0.0147,  ..., -0.0020,  0.0104,  0.0469]],\n",
      "       device='cuda:0'), tensor([[-0.0022, -0.0351,  0.0338,  ...,  0.0547, -0.0067, -0.0338],\n",
      "        [-0.0550,  0.0055,  0.0027,  ..., -0.0429,  0.0159,  0.0413],\n",
      "        [-0.0493, -0.0311,  0.0146,  ..., -0.0013,  0.0859,  0.0423],\n",
      "        ...,\n",
      "        [ 0.0245, -0.0270,  0.0031,  ..., -0.0355,  0.0171, -0.0232],\n",
      "        [-0.0029, -0.0076, -0.0879,  ..., -0.0086, -0.0010, -0.0217],\n",
      "        [ 0.0202,  0.0173, -0.0018,  ...,  0.0125,  0.0175, -0.0071]],\n",
      "       device='cuda:0'), tensor([[ 0.0024, -0.0271, -0.0367,  ..., -0.0409,  0.0133,  0.0423],\n",
      "        [-0.0253, -0.0231, -0.0449,  ...,  0.0028,  0.0098,  0.0288],\n",
      "        [-0.0107,  0.0264,  0.0431,  ..., -0.0395, -0.0006, -0.0144],\n",
      "        ...,\n",
      "        [-0.0323,  0.0128, -0.0283,  ..., -0.0850,  0.0075,  0.0229],\n",
      "        [-0.0984, -0.0102,  0.0244,  ..., -0.0553,  0.0366,  0.0137],\n",
      "        [-0.0225,  0.0197, -0.0216,  ..., -0.0655, -0.0107,  0.0178]],\n",
      "       device='cuda:0'), tensor([[-0.1259,  0.0286,  0.0566,  ..., -0.0623, -0.0261,  0.0287],\n",
      "        [-0.1322,  0.0029,  0.0269,  ..., -0.0651,  0.0071,  0.0316],\n",
      "        [-0.0849, -0.0316,  0.0827,  ..., -0.0432,  0.0123,  0.0200],\n",
      "        ...,\n",
      "        [ 0.0049, -0.0357,  0.0351,  ..., -0.0112,  0.0179,  0.0459],\n",
      "        [-0.0242, -0.0210,  0.0209,  ...,  0.0547, -0.0146,  0.0220],\n",
      "        [ 0.0105, -0.0158,  0.0472,  ...,  0.0292, -0.0087,  0.0409]],\n",
      "       device='cuda:0'), tensor([[ 0.0201, -0.0011,  0.0130,  ...,  0.0684,  0.0119,  0.0832],\n",
      "        [-0.0377, -0.0344,  0.0337,  ...,  0.0620, -0.0068, -0.0006],\n",
      "        [ 0.0179, -0.0336,  0.1121,  ..., -0.0303, -0.0145, -0.0490],\n",
      "        ...,\n",
      "        [-0.0588, -0.0438,  0.0817,  ..., -0.0181,  0.0425, -0.0102],\n",
      "        [-0.0214, -0.0221,  0.0316,  ..., -0.0341, -0.0424, -0.0231],\n",
      "        [-0.0401,  0.0563,  0.0893,  ...,  0.0054,  0.0151, -0.0951]],\n",
      "       device='cuda:0'), tensor([[ 0.0140, -0.0080,  0.0415,  ..., -0.0539, -0.0365, -0.0553],\n",
      "        [-0.0279, -0.0366,  0.0402,  ..., -0.0460,  0.0380, -0.0012],\n",
      "        [ 0.0180, -0.0041,  0.0009,  ..., -0.0032,  0.0047,  0.0167],\n",
      "        ...,\n",
      "        [-0.0508, -0.0392,  0.0716,  ..., -0.0140,  0.0370,  0.0271],\n",
      "        [-0.0034, -0.0248,  0.0286,  ..., -0.0335,  0.0393,  0.0146],\n",
      "        [-0.0285, -0.0687,  0.1094,  ..., -0.0211,  0.0502,  0.0114]],\n",
      "       device='cuda:0'), tensor([[-1.3760e-02, -6.5431e-02,  6.6336e-02,  ..., -3.2957e-02,\n",
      "          1.6995e-02,  5.0479e-02],\n",
      "        [ 1.1532e-03, -4.8650e-02,  1.7188e-02,  ..., -6.4081e-03,\n",
      "          1.0291e-01,  4.7667e-02],\n",
      "        [ 7.9336e-04, -4.1675e-02,  1.1732e-01,  ..., -1.5144e-02,\n",
      "          5.8857e-02,  1.0708e-02],\n",
      "        ...,\n",
      "        [-1.0300e-01, -1.2899e-02,  1.0394e-01,  ..., -7.7130e-05,\n",
      "         -2.5418e-02, -3.5998e-02],\n",
      "        [ 2.1431e-02, -2.1095e-03,  2.3178e-03,  ..., -3.6502e-02,\n",
      "          1.0545e-02, -1.8324e-02],\n",
      "        [-1.3148e-02,  1.5485e-02,  4.3436e-02,  ..., -2.6889e-02,\n",
      "         -2.0691e-02, -3.7512e-03]], device='cuda:0'), tensor([[-0.0320,  0.0168,  0.0866,  ..., -0.0130,  0.0344, -0.0019],\n",
      "        [ 0.0136, -0.0071,  0.0606,  ...,  0.0112,  0.0235,  0.0124],\n",
      "        [ 0.0215,  0.0055,  0.0836,  ..., -0.0499, -0.0062,  0.0730],\n",
      "        ...,\n",
      "        [ 0.0371, -0.0390,  0.0661,  ...,  0.0107,  0.0430, -0.0006],\n",
      "        [ 0.0094, -0.0824,  0.0472,  ...,  0.0441,  0.0449,  0.0091],\n",
      "        [ 0.0339, -0.0276,  0.0355,  ..., -0.0119,  0.0236,  0.0166]],\n",
      "       device='cuda:0'), tensor([[-0.0113, -0.0515,  0.0394,  ...,  0.0138,  0.0601, -0.0075],\n",
      "        [-0.0009, -0.0497,  0.0580,  ...,  0.0244,  0.0505,  0.0621],\n",
      "        [ 0.0712, -0.0489,  0.0731,  ...,  0.0066,  0.0549,  0.0181],\n",
      "        ...,\n",
      "        [ 0.0288,  0.0100,  0.0496,  ..., -0.0123, -0.0279, -0.0122],\n",
      "        [ 0.0365, -0.0429,  0.0186,  ..., -0.0211, -0.0370, -0.0008],\n",
      "        [ 0.0260,  0.0020,  0.0421,  ..., -0.0652, -0.0197, -0.0376]],\n",
      "       device='cuda:0'), tensor([[-0.0441, -0.0009,  0.0767,  ..., -0.0255, -0.0509, -0.0604],\n",
      "        [-0.0024, -0.0350,  0.0399,  ..., -0.0202, -0.0202, -0.0170],\n",
      "        [ 0.0492, -0.0292,  0.0319,  ..., -0.0847, -0.0218,  0.0131],\n",
      "        ...,\n",
      "        [-0.0070, -0.0336,  0.0310,  ..., -0.0330,  0.0190, -0.0295],\n",
      "        [-0.0113, -0.0390,  0.0504,  ..., -0.0062,  0.0147,  0.0258],\n",
      "        [-0.0465, -0.0271,  0.0859,  ...,  0.0298,  0.0235,  0.0195]],\n",
      "       device='cuda:0'), tensor([[-0.0711, -0.0320,  0.0086,  ...,  0.0659,  0.0020, -0.0193],\n",
      "        [-0.0110, -0.0232,  0.0306,  ...,  0.0319, -0.0049,  0.0049],\n",
      "        [-0.0131, -0.0460,  0.0419,  ...,  0.0312,  0.0592,  0.0030],\n",
      "        ...,\n",
      "        [-0.0165,  0.0295, -0.0064,  ..., -0.0013,  0.0314, -0.0223],\n",
      "        [-0.0110, -0.0343,  0.1018,  ...,  0.0045,  0.0251,  0.0107],\n",
      "        [-0.0239, -0.0435,  0.0450,  ...,  0.0079,  0.0708,  0.0079]],\n",
      "       device='cuda:0'), tensor([[-0.0730, -0.0360,  0.1085,  ..., -0.0284,  0.0270, -0.0272],\n",
      "        [-0.0351, -0.0673,  0.0418,  ..., -0.0056,  0.0768, -0.0073],\n",
      "        [-0.0801, -0.0834,  0.1114,  ..., -0.0022,  0.0236, -0.0010],\n",
      "        ...,\n",
      "        [ 0.0128, -0.0140,  0.0431,  ..., -0.0242, -0.0196, -0.0098],\n",
      "        [-0.0478,  0.0006,  0.0291,  ..., -0.0221,  0.0111,  0.0230],\n",
      "        [-0.0159, -0.0401,  0.0508,  ..., -0.0177, -0.0304,  0.0550]],\n",
      "       device='cuda:0'), tensor([[ 0.0103, -0.0382,  0.0170,  ..., -0.0376,  0.0206,  0.0061],\n",
      "        [-0.0144, -0.0439,  0.0251,  ..., -0.0143, -0.0037, -0.0209],\n",
      "        [ 0.0251, -0.0069,  0.0308,  ..., -0.0273,  0.0112, -0.0010],\n",
      "        ...,\n",
      "        [-0.0499, -0.0227,  0.0851,  ..., -0.0233,  0.0178,  0.0056],\n",
      "        [ 0.0214, -0.0339,  0.0494,  ..., -0.0817,  0.0308,  0.0158],\n",
      "        [-0.0683, -0.0166,  0.0445,  ..., -0.0230,  0.0458, -0.0127]],\n",
      "       device='cuda:0'), tensor([[ 0.0165, -0.0138,  0.0362,  ..., -0.0267,  0.0199,  0.0482],\n",
      "        [ 0.0074, -0.0087,  0.0413,  ..., -0.0351, -0.0099,  0.0008],\n",
      "        [-0.0115, -0.0105,  0.0229,  ...,  0.0283, -0.0040,  0.0195],\n",
      "        ...,\n",
      "        [ 0.0244, -0.0187,  0.0614,  ...,  0.0198,  0.0203,  0.0426],\n",
      "        [ 0.0349, -0.0173,  0.0601,  ..., -0.0140, -0.0004,  0.0195],\n",
      "        [ 0.0490, -0.0069,  0.0585,  ...,  0.0013, -0.0097,  0.0002]],\n",
      "       device='cuda:0'), tensor([[ 0.0036,  0.0097,  0.0450,  ..., -0.0378,  0.0178,  0.0436],\n",
      "        [ 0.0069, -0.0228,  0.0179,  ..., -0.0333,  0.0325,  0.0011],\n",
      "        [ 0.0543,  0.0073,  0.0573,  ..., -0.0064,  0.0240,  0.0282],\n",
      "        ...,\n",
      "        [ 0.0136, -0.0139,  0.0188,  ..., -0.0505,  0.0089, -0.0116],\n",
      "        [ 0.0438, -0.0159,  0.0149,  ..., -0.0572,  0.0339,  0.0336],\n",
      "        [ 0.0143, -0.0385,  0.0591,  ...,  0.0089,  0.0321,  0.0164]],\n",
      "       device='cuda:0'), tensor([[-0.0385,  0.0226,  0.0514,  ..., -0.0142,  0.0114, -0.0538],\n",
      "        [ 0.0169, -0.0072,  0.0409,  ...,  0.0096,  0.0086,  0.0203],\n",
      "        [-0.0017, -0.0252,  0.0461,  ..., -0.0505, -0.0232, -0.0122],\n",
      "        ...,\n",
      "        [-0.0105, -0.0278,  0.0868,  ..., -0.0097,  0.0321, -0.0103],\n",
      "        [-0.0113, -0.0681,  0.1259,  ..., -0.0257,  0.0385, -0.0086],\n",
      "        [ 0.0249, -0.0632,  0.0333,  ..., -0.0178,  0.0069,  0.0167]],\n",
      "       device='cuda:0'), tensor([[ 0.0490, -0.0528,  0.0483,  ..., -0.0690,  0.0317,  0.0241],\n",
      "        [-0.0323, -0.0395,  0.1055,  ..., -0.0336,  0.0368,  0.0216],\n",
      "        [ 0.0289, -0.0359,  0.0331,  ..., -0.0459,  0.0310,  0.0319],\n",
      "        ...,\n",
      "        [ 0.0019, -0.0022,  0.0527,  ...,  0.0023,  0.0279,  0.0719],\n",
      "        [ 0.0060, -0.0175,  0.0770,  ...,  0.0311,  0.0279,  0.0307],\n",
      "        [-0.0430, -0.0020,  0.0717,  ...,  0.0086,  0.0364,  0.0374]],\n",
      "       device='cuda:0'), tensor([[ 0.0244, -0.0199,  0.0379,  ..., -0.0225,  0.0743, -0.0173],\n",
      "        [ 0.0177,  0.0005,  0.0659,  ..., -0.0427,  0.0096,  0.0583],\n",
      "        [ 0.0033, -0.0429,  0.0827,  ..., -0.0214,  0.0444, -0.0210],\n",
      "        ...,\n",
      "        [-0.0624, -0.0462,  0.0609,  ..., -0.0264, -0.0488, -0.0470],\n",
      "        [ 0.0269,  0.0109, -0.0008,  ..., -0.0659, -0.0428,  0.0037],\n",
      "        [ 0.0319,  0.0086, -0.0562,  ..., -0.0514, -0.0340, -0.0230]],\n",
      "       device='cuda:0'), tensor([[ 0.0074,  0.0015, -0.0136,  ..., -0.0669,  0.0085, -0.0154],\n",
      "        [ 0.0401, -0.0057,  0.0323,  ..., -0.0716, -0.0046,  0.0214],\n",
      "        [-0.0105, -0.0029,  0.0133,  ..., -0.0569, -0.0102,  0.0150],\n",
      "        ...,\n",
      "        [-0.0952, -0.0453,  0.0404,  ...,  0.0382,  0.0400, -0.0512],\n",
      "        [-0.1069, -0.0214,  0.0537,  ...,  0.0087,  0.0453, -0.0616],\n",
      "        [ 0.0107, -0.0221,  0.0529,  ...,  0.0329, -0.0023,  0.0210]],\n",
      "       device='cuda:0'), tensor([[ 0.0033, -0.0261,  0.0601,  ...,  0.0253, -0.0073,  0.0128],\n",
      "        [ 0.0330, -0.0130,  0.0164,  ..., -0.0091,  0.0248, -0.0064],\n",
      "        [-0.0136, -0.0047,  0.0443,  ...,  0.0015, -0.0004, -0.0297],\n",
      "        ...,\n",
      "        [ 0.0022, -0.0145,  0.0506,  ..., -0.0188, -0.0188,  0.0144],\n",
      "        [ 0.0113, -0.0335,  0.0965,  ..., -0.0604,  0.0113,  0.0118],\n",
      "        [-0.0276, -0.0269,  0.1021,  ..., -0.0157, -0.0286, -0.0147]],\n",
      "       device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(image_features_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a93a1dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m tsne_results = pd.DataFrame(tsne_results)\n\u001b[32m     10\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m scatter = plt.scatter(\u001b[43mtsne_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, tsne_results[:, \u001b[32m1\u001b[39m], c=labels, cmap=\u001b[33m'\u001b[39m\u001b[33mtab10\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m plt.legend(*scatter.legend_elements(), title=\u001b[33m\"\u001b[39m\u001b[33mClasses\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mVisualisation t-SNE des embeddings\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/core/indexes/range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: (slice(None, None, None), 0)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# t-SNE\n",
    "\n",
    "text_features = text_features.to('cpu').detach().numpy()\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(text_features)\n",
    "\n",
    "tsne_results = pd.DataFrame(tsne_results)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='tab10')\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"Visualisation t-SNE des embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051a395",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# t-SNE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m image_feature_tot = \u001b[43mnp\u001b[49m.array(image_feature_tot)\n\u001b[32m      4\u001b[39m tsne = TSNE(n_components=\u001b[32m2\u001b[39m, verbose=\u001b[32m0\u001b[39m, perplexity=\u001b[32m40\u001b[39m, n_iter=\u001b[32m300\u001b[39m)\n\u001b[32m      5\u001b[39m tsne_results = tsne.fit_transform(image_features_tot)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# t-SNE\n",
    "\n",
    "image_feature_tot = np.array(image_feature_tot)\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(image_features_tot)\n",
    "\n",
    "tsne_results = pd.DataFrame(tsne_results)\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=tsne_results,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3,\n",
    "    ax=ax1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
