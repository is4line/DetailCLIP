{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daee20f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.12/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/usr/local/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "# import subprocess\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    wandb = None\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from datasets import get_dataset\n",
    "import models\n",
    "from tokenizer import SimpleTokenizer\n",
    "from utils import AverageMeter, ProgressMeter, accuracy\n",
    "import utils\n",
    "from torchvision.datasets import ImageFolder\n",
    "from utils import GaussianBlur, Solarize\n",
    "from losses import DetailCLIPLoss, get_metric_names\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "648d2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(description='DetailCLIP pre-training and evaluation', add_help=False)\n",
    "    # Data\n",
    "    parser.add_argument('--dataset', default='yfcc15m', type=str, choices=['yfcc15m', 'cc3m', 'cc12m', 'coco', 'redcaps'])\n",
    "    parser.add_argument('--metadata', default='yfcc15m.pkl', type=str,\n",
    "                    help='path to metadata file (see README for details)')\n",
    "    parser.add_argument('--root', default='', type=str,\n",
    "                        help='path to dataset root')\n",
    "    parser.add_argument('--output-dir', default='./', type=str, help='path where to save, empty for no saving')\n",
    "\n",
    "    # Data Augmentation\n",
    "    parser.add_argument('--global_crops_scale', type=float, nargs='+', default=(0.14, 1.),\n",
    "        help=\"\"\"Scale range of the cropped image before resizing, relatively to the origin image.\n",
    "        Used for large global view cropping. When disabling multi-crop (--local_crops_number 0), we\n",
    "        recommand using a wider range of scale (\"--global_crops_scale 0.14 1.\" for example)\"\"\")\n",
    "    # Model\n",
    "    parser.add_argument('--model', default='DetailCLIP_VITB16', type=str)\n",
    "    parser.add_argument('--mask-ratio', default=0.5, type=float)\n",
    "    parser.add_argument('--ssl-mlp-dim', default=4096, type=int,\n",
    "                        help='hidden dim of SimCLR mlp projection head')\n",
    "    parser.add_argument('--ssl-emb-dim', default=256, type=int,\n",
    "                        help='output embed dim of SimCLR mlp projection head')\n",
    "    parser.add_argument('--ssl-scale', default=1.0, type=float,\n",
    "                        help='loss scale for SimCLR objective')\n",
    "    parser.add_argument('--ssl-temp', default=0.1, type=float,\n",
    "                        help='softmax temperature for SimCLR objective')\n",
    "    parser.add_argument('--resume', default='', type=str, help='path to resume from')\n",
    "    # Training\n",
    "    parser.add_argument('--momentum-ema', default=0.996, type=float, help=\"\"\"Base EMA\n",
    "    parameter. The value is increased to 1 during training with cosine schedule.\"\"\")\n",
    "    parser.add_argument('--epochs', default=25, type=int)\n",
    "    parser.add_argument('--warmup-epochs', default=1, type=int)\n",
    "    parser.add_argument('--start-epoch', default=0, type=int)\n",
    "    parser.add_argument('--batch-size', default=5, type=int,\n",
    "                        help='number of samples per-device/per-gpu')\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--base-lr', default=3e-3, type=float)\n",
    "    parser.add_argument('--lr-start', default=1e-6, type=float,\n",
    "                        help='initial warmup lr')\n",
    "    parser.add_argument('--lr-end', default=1e-5, type=float,\n",
    "                        help='minimum final lr')\n",
    "    parser.add_argument('--update-freq', default=1, type=int,\n",
    "                        help='optimizer update frequency (i.e. gradient accumulation steps)')\n",
    "    parser.add_argument('--wd', default=0.5, type=float)\n",
    "    parser.add_argument('--betas', default=(0.9, 0.98), nargs=2, type=float)\n",
    "    parser.add_argument('--eps', default=1e-8, type=float)\n",
    "    parser.add_argument('--eval-freq', default=1, type=int)\n",
    "    parser.add_argument('--disable-amp', action='store_true',\n",
    "                        help='disable mixed-precision training (requires more memory and compute)')\n",
    "    # System\n",
    "    parser.add_argument('--print-freq', default=10, type=int, help='print frequency')\n",
    "    parser.add_argument('-j', '--workers', default=1, type=int, metavar='N',\n",
    "                        help='number of data loading workers per process')\n",
    "    parser.add_argument('--evaluate', action='store_true', help='eval only')\n",
    "    parser.add_argument('--world-size', default=1, type=int,\n",
    "                        help='number of nodes for distributed training')\n",
    "    parser.add_argument('--rank', default=0, type=int,\n",
    "                        help='node rank for distributed training')\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "    parser.add_argument('--dist-url', default='env://', type=str,\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str)\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--gpu', default=None, type=int, help='GPU id to use.')\n",
    "    parser.add_argument('--wandb', action='store_true', help='Enable WandB logging')\n",
    "    parser.add_argument('--descriptions', default='training', type=str)\n",
    "    parser.add_argument('--port', default=29500, help='port of master addr')\n",
    "    # Loss\n",
    "    parser.add_argument('--disable-norm-pix-loss', action='store_true',\n",
    "                        help='disable normalization of pixel loss for reconstruction')\n",
    "    parser.add_argument('--clip_loss_weight', default=1.0, type=float, help='weight of clip loss')\n",
    "    parser.add_argument('--ibot_patch_loss_weight', default=1.0, type=float, help='weight of ibot patch loss')\n",
    "    parser.add_argument('--ibot_cls_loss_weight', default=1.0, type=float, help='weight of ibot classification loss') \n",
    "    parser.add_argument('--reconst_loss_weight', default=1.0, type=float, help='weight of reconstruction loss')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7558c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    print(\"=> creating model: {}\".format(args.model))\n",
    "    model = getattr(models, args.model)(mask_ratio=args.mask_ratio) # note that args.mask_ratio is by default 0\n",
    "    model.cuda(args.gpu)\n",
    "\n",
    "    if args.distributed:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], bucket_cap_mb=200,find_unused_parameters=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fcec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optim(args, model):\n",
    "    p_wd, p_non_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue  # frozen weights\n",
    "        if p.ndim < 2 or 'bias' in n or 'ln' in n or 'bn' in n:\n",
    "            p_non_wd.append(p)\n",
    "        else:\n",
    "            p_wd.append(p)\n",
    "\n",
    "    optim_params = [{\"params\": p_wd, \"weight_decay\": args.wd},\n",
    "                    {\"params\": p_non_wd, \"weight_decay\": 0},\n",
    "                    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(optim_params, lr=args.lr, betas=args.betas,\n",
    "                                    eps=args.eps, weight_decay=args.wd)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372caec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckpt(args, model, optimizer, scaler):\n",
    "    # optionally resume from a checkpoint (takes precedence over autoresume)\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading resume checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "            epoch = checkpoint['epoch'] if 'epoch' in checkpoint else 0\n",
    "            args.start_epoch = epoch\n",
    "            result = model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "            print(result)\n",
    "            optimizer.load_state_dict(checkpoint['optimizer']) if 'optimizer' in checkpoint else ()\n",
    "            scaler.load_state_dict(checkpoint['scaler']) if 'scaler' in checkpoint else ()\n",
    "            args.best_acc = checkpoint['best_acc']\n",
    "            print(\"=> loaded resume checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, epoch))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "    else:\n",
    "        # auto-resume from latest checkpoint in output directory\n",
    "        latest = os.path.join(args.output_dir, 'checkpoint.pt')\n",
    "        if os.path.isfile(latest):\n",
    "            print(\"=> loading latest checkpoint '{}'\".format(latest))\n",
    "            latest_checkpoint = torch.load(latest, map_location='cpu')\n",
    "            args.start_epoch = latest_checkpoint['epoch']\n",
    "            model.load_state_dict(latest_checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(latest_checkpoint['optimizer'])\n",
    "            scaler.load_state_dict(latest_checkpoint['scaler'])\n",
    "            args.best_acc = latest_checkpoint['best_acc']\n",
    "            print(\"=> loaded latest checkpoint '{}' (epoch {})\"\n",
    "                  .format(latest, latest_checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dea368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(args, tokenizer):\n",
    "    print(\"=> creating dataset\")\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=args.global_crops_scale, interpolation=3), # 3 is bicubic\n",
    "        transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)  # not strengthened\n",
    "        ], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.1),\n",
    "        transforms.RandomApply([Solarize()], p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    train_dataset = get_dataset(train_transform, tokenizer, args)\n",
    "    cwd = os.path.dirname(os.path.realpath(__file__))\n",
    "    with open(os.path.join(cwd, 'dataset_catalog.json')) as f:\n",
    "        root = json.load(f)['imagenet']['path']\n",
    "    #add val folder for imagenet 1k\n",
    "    val_dataset = ImageFolder(os.path.join(root, 'val'), val_transform)\n",
    "\n",
    "    # dist eval resamples data to pad uneven batch sizes\n",
    "    # make sure num_samples = 0 mod num_gpus for exact acc\n",
    "    if args.distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        val_sampler = None\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True, sampler=train_sampler, drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.batch_size, shuffle=(val_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True, sampler=val_sampler, drop_last=False)\n",
    "    \n",
    "    return train_loader, train_sampler, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7e58430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, scaler, epoch, lr_schedule, momentum_schedule, args):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.2f')\n",
    "    mem = AverageMeter('Mem (GB)', ':6.3f')\n",
    "    metric_names = get_metric_names()\n",
    "    iters_per_epoch = len(train_loader) // args.update_freq\n",
    "    metrics = OrderedDict([(name, AverageMeter(name, ':.2e')) for name in metric_names])\n",
    "    progress = ProgressMeter(\n",
    "        iters_per_epoch,\n",
    "        [batch_time, data_time, mem, *metrics.values()],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for data_iter, inputs in enumerate(train_loader):\n",
    "        optim_iter = data_iter // args.update_freq\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # update weight decay and learning rate according to their schedule\n",
    "        it = iters_per_epoch * epoch + optim_iter  # global training iteration\n",
    "        for k, param_group in enumerate(optimizer.param_groups):\n",
    "            param_group['lr'] = lr_schedule[it]\n",
    "        \n",
    "        online_inputs = [inputs[0][0], inputs[0][1], inputs[1]]\n",
    "        online_inputs = [tensor.cuda(args.gpu, non_blocking=True) for tensor in online_inputs]\n",
    "\n",
    "        m = momentum_schedule[it]  # momentum parameter\n",
    "        # compute output\n",
    "        with amp.autocast(enabled=not args.disable_amp):\n",
    "            outputs = model(*online_inputs, m)\n",
    "            loss_dict = criterion(outputs, epoch)\n",
    "\n",
    "            loss = loss_dict['loss']\n",
    "            loss /= args.update_freq\n",
    "\n",
    "        if not math.isfinite(loss.item()):\n",
    "            print(\"Loss is {}, stopping training\".format(loss.item()))\n",
    "            sys.exit(1)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (data_iter + 1) % args.update_freq != 0:\n",
    "            continue\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        # clamp logit scale to [0, 100]\n",
    "        logit_scale_e = 0\n",
    "\n",
    "        utils.get_model(model).logit_scale.data.clamp_(0, 4.6052)   # since logit_scale is defined within the model class it is a learnable parameter that needs to be clamped\n",
    "        if hasattr(utils.get_model(model),'logit_scale_e'):\n",
    "            utils.get_model(model).logit_scale_e.data.clamp_(0, 4.6052)\n",
    "            logit_scale_e = utils.get_model(model).logit_scale_e.exp().item()\n",
    "\n",
    "        logit_scale = utils.get_model(model).logit_scale.exp().item()\n",
    "\n",
    "        for k in loss_dict:\n",
    "            metrics[k].update(loss_dict[k].item(), args.batch_size)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "        mem.update(torch.cuda.max_memory_allocated() / 1e9)\n",
    "        if optim_iter % args.print_freq == 0:\n",
    "            if utils.is_main_process() and args.wandb:\n",
    "                wandb.log({**{k: v.item() for k, v in loss_dict.items()},\n",
    "                        'scaler': scaler.get_scale(),\n",
    "                        'logit': logit_scale,\n",
    "                        'logit_e': logit_scale_e,\n",
    "                        })\n",
    "            progress.display(optim_iter)\n",
    "\n",
    "    progress.synchronize()\n",
    "    return {**{k: v.avg for k, v in metrics.items()},\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "            'logit_scale': logit_scale}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
